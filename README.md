# ğŸ”¥ Transformer Playground

A modular, well-documented implementation of the original **Transformer** architecture from [Attention is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762), built using **PyTorch**.

Developed by **xiyuan.chen**, this project is designed for educational purposes and experimentation with different attention mechanisms and transformer variants.

---

## ğŸ“Œ Features

- âœ… Encoder-Decoder Transformer (from scratch)
- ğŸ§  Multi-Head Attention (with mask support)
- ğŸ—ï¸ Positional Encoding (sinusoidal)
- ğŸ“ˆ Teacher Forcing Training Mode
- ğŸ“Š BLEU Score Evaluation
- ğŸ” Custom Dataset Support (Text-to-Text)
- ğŸ” Visualization: Attention Weights with `matplotlib`
- ğŸš€ Transformer Variants: Add & compare `GPT`, `BERT`, `Linformer`, etc. (WIP)

---

## ğŸ“¦ Requirements

This project uses the following dependencies:

```bash
pip install torch torchvision
pip install numpy matplotlib
pip install tqdm scikit-learn
pip install nltk
```

##ğŸš€ Quick Start

from transformer.model import Transformer
from transformer.utils import train_model, evaluate_model

model = Transformer(
    src_vocab_size=8000,
    tgt_vocab_size=8000,
    d_model=512,
    num_heads=8,
    num_encoder_layers=6,
    num_decoder_layers=6,
    dim_feedforward=2048,
    dropout=0.1
)

train_model(model, dataloader, optimizer, loss_fn, num_epochs=10)
evaluate_model(model, test_loader)

##ğŸ§  Architecture Overview

Input Embedding â†’ Positional Encoding
   
    â†“
Multi-Head Attention â†’ Add & Norm

      â†“
Feed Forward â†’ Add & Norm

      â†“
       â†“           â† Skip Connection â†’
       
Decoder Embedding â†’ Positional Encoding

      â†“
Masked Multi-Head Attention â†’ Add & Norm

      â†“
Encoder-Decoder Attention â†’ Add & Norm

      â†“
Feed Forward â†’ Add & Norm

      â†“
Linear + Softmax

##ğŸ“š Training Configuration

python train.py --dataset data/eng-fra.txt --epochs 20 --batch_size 64 --lr 0.0005

##ğŸ“ Folder Structure

transformer/

â”œâ”€â”€ model.py         # Transformer model

â”œâ”€â”€ layers.py        # Attention and Feedforward layers

â”œâ”€â”€ utils.py         # Training helpers and evaluation

â”œâ”€â”€ train.py         # Training entry point

â”œâ”€â”€ evaluate.py      # Evaluation script

â”œâ”€â”€ data/            # Raw and processed datasets

â””â”€â”€ checkpoints/     # Saved models


##ğŸ“– References
Vaswani, A., et al. (2017). Attention is All You Need.

The Annotated Transformer: https://nlp.seas.harvard.edu/2018/04/03/attention.html

PyTorch Tutorials: https://pytorch.org/tutorials/

ğŸ‘¤ Author

xiyuan.chen

